{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepcocessing example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is the most important stage in whole data life cycle.\n",
    "If you're working with data you know that's very rare that we're getting clean and well structured data 'first-hand'.\n",
    "In this article i'm going to show you how to preprocess data using pandas.\n",
    "I'm going to use dataset which contains flat rent offers scraped from one of the polish website for one particular town which is Poznań.\n",
    "So here's the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach when I'm facing problem like this is using unique method on pandas series and simply printing and examining unique values in every column just by glance.\n",
    "We can achieve this using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column)\n",
    "    print(df[column].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i' m going to explain preprocessing steps for every column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create simple function which iterates over every element in string and joins only values which are numeric, if value is None (empty value) function will return [np.nan](https://numpy.org/doc/stable/reference/constants.html?highlight=nan#numpy.nan):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_values_from_string(s):\n",
    "    if s is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(''.join([v for v in s if v.isnumeric()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_numeric_values_from_string():\n",
    "    s = '800 zł'\n",
    "    actual = get_numeric_values_from_string(s)\n",
    "    expected = 800\n",
    "    assert actual == expected\n",
    "\n",
    "test_get_numeric_values_from_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finaly using map method in order to use function written above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fees = df.fees.map(get_numeric_values_from_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number_of_rooms, year_built and number_of_parking_spaces**\n",
    "\n",
    "\n",
    "Let's look at data types before we do anything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns number_of_rooms, year_built and number_of_parking_spaces should be numeric, why ?\n",
    "\n",
    "lets look at unique values in this columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['number_of_rooms', 'year_built', 'number_of_parking_spaces']\n",
    "\n",
    "for column_to_check in columns_to_check:\n",
    "    print(column_to_check)\n",
    "    print(df[column_to_check].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create python dictionary where key will be name of the column we want to convert and the value will be datatype this column should be converted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    'number_of_rooms': float, \n",
    "    'year_built': float, \n",
    "    'number_of_parking_spaces': float\n",
    "}\n",
    "\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data types now looks like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number_of_floors_in_the_building and floor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace some values in this two columns\n",
    "In order to do so im going to create python dictionary which will be used as mapper, key from this dictionary is the value we want to replace and value of this key is new value we want to set. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper:dict = {\n",
    "    '0 (parter)': 0\n",
    "}\n",
    "df.number_of_floors_in_the_building = df.number_of_floors_in_the_building.replace(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same approach for floor column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper:dict = {\n",
    "    'parter': 0,\n",
    "    'low parter':0\n",
    "}\n",
    "df.floor = df.floor.replace(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast look at datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    'number_of_floors_in_the_building': float, \n",
    "    'floor': float, \n",
    "}\n",
    "\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Let's go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**area_in_m2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach is to create new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_area_in_m2_to_numeric(s):\n",
    "    return float(s.replace('m2', '').replace(',', '.').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write simple test for this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_area_in_m2_to_numeric():\n",
    "    s= '100,57 m2'\n",
    "    actual= convert_area_in_m2_to_numeric(s)\n",
    "    expected = 100.57\n",
    "    assert actual == expected\n",
    "    \n",
    "test_convert_area_in_m2_to_numeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally use map method in order to apply it on pandas serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.area_in_m2 = df.area_in_m2.map(convert_area_in_m2_to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can use use vectorized string methods on pandas serie, and write it like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.area_in_m2 = df.area_in_m2.str.replace('m2', '').str.replace(',', '.').str.strip().astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we re going to use this for instance in ETL pipeline my recommendation is to define function, the biggest pros of writing a function is that we can test it, which is extremaly important !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**location**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last column which should be preprocessed is location, again two approaches write function and test it and use vectorized function\n",
    "\n",
    "first approach:\n",
    "\n",
    "new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(s):\n",
    "    return ' '.join(s.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_location() -> None:\n",
    "    s = '''\n",
    "\n",
    "    Poznań,       Stare Miasto,    wielkopolskie\n",
    "    \n",
    "    '''\n",
    "    actual = get_location(s)\n",
    "    expected = 'Poznań, Stare Miasto, wielkopolskie'\n",
    "    assert actual == expected\n",
    "    \n",
    "test_get_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use map method for pandas serie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location = df.location.map(get_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second approach (not recommended by me but also valid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.location = df.location.str.strip().str.split().apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use unique method in order to see locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the values - Poznań, wielkopolskie doesn't give any information - we know that we re dealing with flats from Poznań, wielkopolskie is name voivodeship. Let's replace it with [np.nan](https://numpy.org/doc/stable/reference/constants.html?highlight=nan#numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location = df.location.replace({'Poznań, wielkopolskie': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing we want to do is to extract district from location, we can do it easily by picking second element (first index) at splited location, to do so let's define new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_district(s):\n",
    "    if s is np.nan:\n",
    "        return np.nan\n",
    "    else:\n",
    "        splited:list = s.split(', ')\n",
    "        return splited[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course **test it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_district():\n",
    "    s = 'Poznań, Stare Miasto, wielkopolskie'\n",
    "    actual = get_district(s)\n",
    "    expected = 'Stare Miasto'\n",
    "    assert actual == expected\n",
    "    \n",
    "test_get_district()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['district'] = df.location.map(get_district)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray!\n",
    "\n",
    "Preprocessing has been finished and final datset looks great, let's look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now further use cases for this dataset are almost **unlimited**, examples:\n",
    "\n",
    "* building machine learning model\n",
    "* store in data warehouse and use it for BI\n",
    "* data visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2c62667458288d5907abca8bdbf4dce0afea8e41911bca3184aa0b6a74b7e60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
